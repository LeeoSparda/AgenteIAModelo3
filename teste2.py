# llamaV2_fix.py
import os
import glob
import sys
import json

# ---------- Imports resilientes (tenta m√∫ltiplos caminhos) ----------
print("üîé Tentando importar depend√™ncias do LangChain/FAISS/Ollama...")

# Document loaders
PyPDFLoader = None
TextLoader = None
Docx2txtLoader = None
try:
    from langchain_community.document_loaders import PyPDFLoader, TextLoader, Docx2txtLoader
    print("‚úîÔ∏è Usando langchain_community.document_loaders")
except Exception:
    try:
        from langchain.document_loaders import PyPDFLoader, TextLoader, Docx2txtLoader
        print("‚úîÔ∏è Usando langchain.document_loaders")
    except Exception:
        print("‚ùå N√£o encontrou loaders document_loaders automaticamente. Instale langchain-community.")
        # N√£o aborta aqui; o erro aparecer√° se tentar usar loaders.

# Text splitter
try:
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    print("‚úîÔ∏è Usando langchain_text_splitters.RecursiveCharacterTextSplitter")
except Exception:
    try:
        from langchain.text_splitter import RecursiveCharacterTextSplitter
        print("‚úîÔ∏è Usando langchain.text_splitter.RecursiveCharacterTextSplitter")
    except Exception:
        RecursiveCharacterTextSplitter = None
        print("‚ùå RecursiveCharacterTextSplitter n√£o encontrado - instale langchain-text-splitters")

# Vectorstore FAISS
FAISS = None
try:
    from langchain_community.vectorstores import FAISS
    print("‚úîÔ∏è Usando langchain_community.vectorstores.FAISS")
except Exception:
    try:
        from langchain.vectorstores import FAISS
        print("‚úîÔ∏è Usando langchain.vectorstores.FAISS")
    except Exception:
        FAISS = None
        print("‚ùå FAISS import n√£o encontrado - instale langchain-community e faiss-cpu")

# Embeddings (prefer sentence-transformers)
HuggingFaceEmbeddings = None
try:
    from langchain.embeddings import HuggingFaceEmbeddings
    print("‚úîÔ∏è Usando langchain.embeddings.HuggingFaceEmbeddings")
except Exception:
    try:
        from langchain_hub import HuggingFaceEmbeddings
        print("‚úîÔ∏è Usando langchain_hub.HuggingFaceEmbeddings (fallback)")
    except Exception:
        HuggingFaceEmbeddings = None
        print("‚ùå HuggingFaceEmbeddings n√£o encontrado - instale sentence-transformers e langchain embeddings")

# Ollama LLM
OllamaLLM = None
try:
    from langchain_ollama import OllamaLLM
    print("‚úîÔ∏è Usando langchain_ollama.OllamaLLM")
except Exception:
    try:
        # fallback: older names
        from langchain_ollama import Ollama as OllamaLLM
        print("‚úîÔ∏è Usando langchain_ollama.Ollama (alias)")
    except Exception:
        OllamaLLM = None
        print("‚ùå langchain_ollama n√£o dispon√≠vel - instale langchain-ollama or use HF model")

# Retrieval chain (compatibilidade)
RetrievalQA = None
create_retrieval_chain = None
try:
    # primeira tentatica: import direto (algumas vers√µes)
    from langchain.chains import RetrievalQA
    print("‚úîÔ∏è Usando langchain.chains.RetrievalQA")
    RetrievalQA = RetrievalQA
except Exception:
    try:
        from langchain.chains.retrieval_qa.base import RetrievalQA
        print("‚úîÔ∏è Usando langchain.chains.retrieval_qa.base.RetrievalQA")
        RetrievalQA = RetrievalQA
    except Exception:
        try:
            from langchain.chains import create_retrieval_chain
            print("‚úîÔ∏è Usando langchain.chains.create_retrieval_chain")
            create_retrieval_chain = create_retrieval_chain
        except Exception:
            print("‚ùå RetrievalQA / create_retrieval_chain n√£o encontrados - instale langchain atualizado")

# ---------- Verifica√ß√µes m√≠nimas ----------
if RecursiveCharacterTextSplitter is None or FAISS is None or HuggingFaceEmbeddings is None:
    print("\n‚ùó Aviso: Seu ambiente est√° faltando algumas bibliotecas essenciais.")
    print("Sugest√£o: pip install -U langchain langchain-core langchain-community langchain-text-splitters langchain-ollama faiss-cpu pypdf python-docx sentence-transformers\n")
    # N√£o abortamos; o script tentar√° rodar e mostrar√° erros mais claros.

# ---------- Configura√ß√µes ----------
PASTA_DOCUMENTOS = r"C:\Users\Leonardo Borges\Desktop\ChatBotIA\Modelo3-LLAMA\Base"  # <- ajuste aqui
if not os.path.exists(PASTA_DOCUMENTOS):
    os.makedirs(PASTA_DOCUMENTOS)
    print(f"üìÅ Pasta criada em: {PASTA_DOCUMENTOS}. Coloque seus .pdf/.docx/.txt l√° e reexecute.")
    sys.exit(0)

# ---------- Carregar documentos ----------
def carregar_docs(pasta):
    docs = []
    patterns = ["*.pdf", "*.docx", "*.txt"]
    for p in patterns:
        for caminho in glob.glob(os.path.join(pasta, p)):
            print("üìÑ Lendo:", caminho)
            try:
                if caminho.lower().endswith(".pdf") and 'PyPDFLoader' in globals() and PyPDFLoader is not None:
                    docs.extend(PyPDFLoader(caminho).load())
                elif caminho.lower().endswith(".docx") and 'Docx2txtLoader' in globals() and Docx2txtLoader is not None:
                    docs.extend(Docx2txtLoader(caminho).load())
                elif caminho.lower().endswith(".txt") and 'TextLoader' in globals() and TextLoader is not None:
                    docs.extend(TextLoader(caminho).load())
                else:
                    # Fallback: tenta leitura simples
                    with open(caminho, "r", encoding="utf-8", errors="ignore") as f:
                        text = f.read()
                        docs.append({"page_content": text, "metadata": {"source": caminho}})
            except Exception as e:
                print("‚ö†Ô∏è Erro ao ler", caminho, ":", e)
    return docs

documentos = carregar_docs(PASTA_DOCUMENTOS)
if not documentos:
    print("‚ö†Ô∏è Nenhum documento carregado. Verifique a pasta e os loaders.")
    sys.exit(1)

print(f"‚úÖ {len(documentos)} blocos/documentos carregados (antes do split).")

# ---------- Split em chunks ----------
if RecursiveCharacterTextSplitter is None:
    print("‚ùó Sem text splitter instalado, n√£o ser√° feito split. Siga a sugest√£o de instala√ß√£o.")
    docs_chunks = documentos
else:
    splitter = RecursiveCharacterTextSplitter(chunk_size=900, chunk_overlap=150)
    # se documentos j√° estiverem no formato de Document do langchain, splitter espera esse formato
    # tratamos caso sejam dicts: transformamos em objeto simples
    from types import SimpleNamespace
    docs_for_split = []
    for d in documentos:
        if isinstance(d, dict):
            docs_for_split.append(SimpleNamespace(page_content=d.get("page_content",""), metadata=d.get("metadata",{})))
        else:
            docs_for_split.append(d)
    docs_chunks = splitter.split_documents(docs_for_split)
    print(f"üìÑ Ap√≥s split: {len(docs_chunks)} partes.")

# ---------- Embeddings e FAISS ----------
if HuggingFaceEmbeddings is None or FAISS is None:
    print("‚ùó N√£o √© poss√≠vel criar FAISS/Embeddings: libs faltando.")
    sys.exit(1)

print("üß¨ Criando embeddings (sentence-transformers/all-MiniLM-L6-v2)...")
emb = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

print("üíæ Indexando no FAISS...")
banco = FAISS.from_documents(docs_chunks, emb)
print("‚úÖ Index criado.")

# ---------- Configurar LLM Ollama (ou fallback para erro claro) ----------
if OllamaLLM is None:
    print("‚ùó OllamaLLM n√£o encontrado. Verifique instala√ß√£o langchain-ollama. Tentando continuar (mas n√£o haver√° LLM).")
    sys.exit(1)

modelo_ollama = "phi3"  # altere aqui para 'phi3' ou 'llama3', etc.
print(f"üîÅ Inicializando OllamaLLM (modelo={modelo_ollama})...")
llm = OllamaLLM(model=modelo_ollama)

# ---------- Criar a cadeia de retrieval (compat√≠vel com v√°rias vers√µes) ----------
if RetrievalQA is not None:
    try:
        qa = RetrievalQA.from_chain_type(llm=llm, retriever=banco.as_retriever(search_kwargs={"k":3}), return_source_documents=True)
        print("‚úîÔ∏è RetrievalQA criado via from_chain_type.")
    except Exception as e:
        print("‚ö†Ô∏è Falha ao criar RetrievalQA via from_chain_type:", e)
        qa = None
elif create_retrieval_chain is not None:
    try:
        # fallback usando create_retrieval_chain
        from langchain.chains.combine_documents import create_stuff_documents_chain
        combine = create_stuff_documents_chain(llm)
        qa = create_retrieval_chain(banco.as_retriever(search_kwargs={"k":3}), combine)
        print("‚úîÔ∏è Retrieval chain criado via create_retrieval_chain.")
    except Exception as e:
        print("‚ùå Falha ao criar chain via create_retrieval_chain:", e)
        qa = None
else:
    print("‚ùå N√£o foi poss√≠vel criar o chain de recupera√ß√£o. Verifique langchain.")
    qa = None

if qa is None:
    print("‚ùó Erro: cadeia de QA n√£o inicializada. Saindo.")
    sys.exit(1)

# ---------- Loop de intera√ß√£o ----------
print("\nü§ñ Base pronta. Pergunte algo (digite 'fim' para sair):\n")
while True:
    q = input("Voc√™: ").strip()
    if q.lower() in ("fim","sair","exit"):
        print("üëã Ok, tchau.")
        break
    print("üîé Buscando trecho relevante...")
    try:
        resposta = qa({"query": q})
    except Exception as e:
        print("‚ùå Erro ao executar qa:", e)
        continue

    # resposta pode ser dict com 'result' ou 'answer' dependendo da vers√£o
    answer = resposta.get("result") or resposta.get("answer") or resposta
    print("\nAssistente:", answer)
    print("-"*50)
    sources = resposta.get("source_documents") or resposta.get("source_documents", [])
    if sources:
        for i, s in enumerate(sources, 1):
            src = getattr(s, "metadata", None)
            if isinstance(src, dict):
                srcname = src.get("source", "Desconhecida")
            else:
                srcname = getattr(s, "metadata", {}).get("source", "Desconhecida")
            print(f"Fonte {i}: {srcname}")
    print("-"*50)