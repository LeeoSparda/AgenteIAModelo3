README â€“ Sistema de Perguntas e Respostas com LLama + FAISS + LangChain

Este projeto implementa um chat inteligente baseado em documentos locais, utilizando:

LangChain

FAISS (vetor store)

Embeddings da HuggingFace (MiniLM)

Modelo LLama rodando via Ollama

PDFs, TXT e DOCX como base de conhecimento

VocÃª adiciona os arquivos em uma pasta, o sistema cria embeddings, monta um banco vetorial e responde perguntas baseadas no conteÃºdo.

ğŸ§© 1. Requisitos do Sistema
âœ” Python

Python 3.9+

âœ” DependÃªncias externas obrigatÃ³rias

Ollama instalado
Download: https://ollama.com/download

ApÃ³s instalar, rode:

ollama run llama3


ou outro modelo (phi3, gemma, mistral, etc.)

âœ” GPU (opcional, mas recomendado)

NVIDIA / AMD ou Apple Silicon

Ollama acelera automaticamente se GPU estiver disponÃ­vel

ğŸ“ 2. Estrutura de Arquivos do Projeto
/seu-projeto
â”‚
â”œâ”€â”€ llamaV2.py                      â†’ script principal
â”œâ”€â”€ Base/                           â†’ pasta com seus documentos
â”‚   â”œâ”€â”€ arquivo1.pdf
â”‚   â”œâ”€â”€ arquivo2.docx
â”‚   â”œâ”€â”€ arquivo3.txt
â”‚   â””â”€â”€ ...
â””â”€â”€ README.md

ğŸ“‚ 3. Criar a Pasta Base de Conhecimento

O script usa esta pasta:

C:/Users/Leonardo Borges/Desktop/ChatBotIA/Modelo3-LLAMA/Base/


Mas vocÃª pode mudar o caminho alterando:

PASTA_DOCUMENTOS = "C:/.../Base/"


Dentro dela, coloque arquivos:

.pdf

.docx

.txt

Se a pasta estiver vazia, o script avisa e encerra.

ğŸ“¦ 4. Instalar DependÃªncias Python

Instale os pacotes necessÃ¡rios:

pip install langchain langchain-community langchain-ollama faiss-cpu sentence-transformers pypdf python-docx


Se quiser usar FAISS GPU:

pip install faiss-gpu

ğŸ”§ 5. O que o Script Faz

O arquivo llamaV2.py executa todo o pipeline de RAG (Retrieval Augmented Generation):

ğŸ“Œ 1. Carrega os documentos

Usando loaders:

PDF â†’ PyPDFLoader

DOCX â†’ Docx2txtLoader

TXT â†’ TextLoader

ğŸ“Œ 2. Divide em chunks

Trechos de:

chunk_size=1000

chunk_overlap=100

ğŸ“Œ 3. Gera embeddings

Usando:

sentence-transformers/all-MiniLM-L6-v2


RÃ¡pido e eficiente.

ğŸ“Œ 4. Cria o banco vetorial FAISS

Armazena embeddings localmente para buscas rÃ¡pidas.

ğŸ“Œ 5. Usa o modelo LLama no Ollama
modelo_ollama = "llama3"
llm = OllamaLLM(model=modelo_ollama)


VocÃª pode trocar por:

mistral

phi3

neural-chat

gemma

llama3:instruct

etc.

ğŸ“Œ 6. Cria o sistema de QA com LangChain

O sistema faz:

Busca semÃ¢ntica no FAISS

Seleciona trechos relevantes

Envia ao modelo LLama gerar a resposta

Retorna resposta + fontes

ğŸ“Œ 7. Inicia o chat interativo

Comandos para sair:

sair

fim

exit

ğŸš€ 6. Como Executar o Script

No terminal:

python llamaV2.py


Se estiver tudo certo, verÃ¡:

ğŸ“„ Total de X partes criadas.
ğŸ¤– Base de conhecimento pronta!


E poderÃ¡ perguntar:

VocÃª: O que diz o contrato sobre rescisÃ£o?

ğŸ’¡ 7. Personalizar o Modelo

No cÃ³digo:

modelo_ollama = "llama3"


Substitua por qualquer modelo instalado no Ollama:

ollama pull phi3
ollama pull mistral
ollama pull gemma
