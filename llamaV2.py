import os
import glob
import json
import requests
from langchain_community.document_loaders import PyPDFLoader, TextLoader, Docx2txtLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_ollama import OllamaLLM
from langchain.chains import RetrievalQA
from langchain.embeddings import HuggingFaceEmbeddings

# ================================
# üóÇÔ∏è 1. CONFIGURA√á√ÉO DA BASE LOCAL
# ================================

PASTA_DOCUMENTOS = "C:/Users/Leonardo Borges/Desktop/ChatBotIA/Modelo3-LLAMA/Base/"  # pasta onde ficar√£o seus arquivos .pdf, .docx, .txt

if not os.path.exists(PASTA_DOCUMENTOS):
    os.makedirs(PASTA_DOCUMENTOS)
    print(f"üìÇ Pasta '{PASTA_DOCUMENTOS}' criada. Adicione seus arquivos nela e execute novamente.")
    exit()

documentos = []

for arquivo in os.listdir(PASTA_DOCUMENTOS):
    caminho = os.path.join(PASTA_DOCUMENTOS, arquivo)
    if arquivo.endswith(".pdf"):
        documentos.append(PyPDFLoader(caminho).load())
    elif arquivo.endswith(".txt"):
        documentos.append(TextLoader(caminho).load())
    elif arquivo.endswith(".docx"):
        documentos.append(Docx2txtLoader(caminho).load())

# Achata a lista (pois cada loader retorna uma lista)
documentos = [doc for sublist in documentos for doc in sublist]

if not documentos:
    print(f"‚ö†Ô∏è Nenhum documento encontrado na pasta '{PASTA_DOCUMENTOS}'.")
    exit()

print(f"‚úÖ {len(documentos)} documentos carregados da pasta '{PASTA_DOCUMENTOS}'.")

# ==================================
# ‚úÇÔ∏è 2. DIVIS√ÉO EM PARTES (CHUNKS)
# ==================================
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
docs_divididos = splitter.split_documents(documentos)
print(f"üìÑ Total de {len(docs_divididos)} partes criadas.")

# ==================================
# üß¨ 3. GERA√á√ÉO DE EMBEDDINGS
# ==================================
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# ==================================
# üß† 4. CRIA√á√ÉO DO BANCO VETORIAL
# ==================================
banco = FAISS.from_documents(docs_divididos, embeddings)

# ==================================
# üó£Ô∏è 5. CONFIGURA√á√ÉO DO MODELO OLLAMA
# ==================================
modelo_ollama = "llama3"  # altere aqui para outro modelo (ex: phi3, mistral, gemma, neural-chat, etc.)
llm = OllamaLLM(model=modelo_ollama)

# ==================================
# üîé 6. CRIA√á√ÉO DO SISTEMA DE PERGUNTAS
# ==================================
qa = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=banco.as_retriever(search_kwargs={"k": 3}),
    return_source_documents=True
)

print("\nü§ñ Base de conhecimento pronta! Agora posso te ajudar com base nos arquivos carregados.\n")

# ==================================
# üí¨ 7. LOOP DE CHAT INTERATIVO
# ==================================
while True:
    pergunta = input("Voc√™: ").strip()
    if pergunta.lower() in ["sair", "fim", "exit"]:
        print("üëã At√© logo!")
        break

    resposta = qa(pergunta)
    print("\nAssistente:", resposta["result"])
    print("-" * 50)
